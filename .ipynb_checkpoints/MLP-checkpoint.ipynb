{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class mlp(object):\n",
    "    def __init__(self, Layers=(2, 5, 3), BatchSize = 4, activation=\"ReLU\", leakyRate=0.01):\n",
    "        self.bs = BatchSize\n",
    "        self.activation = activation\n",
    "        self.leakyRate = leakyRate\n",
    "        self.net=[]\n",
    "        for i in range(len(Layers)):\n",
    "            if i == 0 :\n",
    "                self.net.append(fl(layer=Layers[0], pre_layer=Layers[0], bs = self.bs))\n",
    "            else:\n",
    "                self.net.append(fl(layer=Layers[i], pre_layer=Layers[i-1], bs = self.bs))\n",
    "        self.p = np.zeros(self.net[-1].a.shape,dtype='float64')\n",
    "        self.dJdp = np.zeros(self.p.shape,dtype='float64')\n",
    "        self.yhat = np.zeros(self.bs,dtype=int)#predict answer, presented by probility\n",
    "        self.y_predict = np.zeros(self.p.shape,dtype=int)#one-hot encodeed\n",
    "        \n",
    "        self.J = [] #Loss value trace\n",
    "        self.J_val = []#Loss value trace for validation\n",
    "        self.L2_regularization=[]\n",
    "              \n",
    "    def forward(self,x):\n",
    "        np.copyto(self.net[0].i,x)\n",
    "        np.copyto(self.net[0].z,self.net[0].i)\n",
    "        np.copyto(self.net[0].bnz,self.net[0].z)\n",
    "        np.copyto(self.net[0].a,self.net[0].bnz)\n",
    "                  \n",
    "        for i in range(1,len(self.net)):\n",
    "            np.copyto(self.net[i].i, self.net[i-1].a)\n",
    "            np.copyto(self.net[i].z, np.dot(self.net[i].W,self.net[i].i) + self.net[i].b)\n",
    "            np.copyto(self.net[i].bnz,self.batch_norm(self.net[i].z,\n",
    "                                                      self.net[i].gamma,\n",
    "                                                      self.net[i].beta,\n",
    "                                                      0.01))\n",
    "            self.activation_func(i=i)\n",
    "        np.copyto(self.p,self.softmax(self.net[-1].a))\n",
    "        np.copyto(self.yhat,np.argmax(self.p,axis = 0))\n",
    "       \n",
    "        return\n",
    "                  \n",
    "    # Batch Normalization for an MLP Layer\n",
    "    # x --> bnx              \n",
    "    def batch_norm(self, x, gamma, beta, eps):\n",
    "        \n",
    "        bs = x.shape[-1]\n",
    "        mu = 1.0/bs * np.sum(x, axis=-1)[:, None]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=-1)[:, None]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        bnx = gamma * xhat + beta\n",
    "        \n",
    "        return bnx\n",
    "    \n",
    "    # Backprop Batch Normalization for an MLP Layer\n",
    "    # (dJdbnx, x, gamma, beta) --> (dJdbeta, dJdgamma, dJdx)\n",
    "    def batch_norm_prime(self, dJdbnx, x, gamma, beta, eps):\n",
    "        \n",
    "        bs = x.shape[-1]\n",
    "        mu = 1.0/bs * np.sum(x, axis=-1)[:, None]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=-1)[:, None]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        dJdbeta = np.mean(dJdbnx, axis=-1)[:, None]\n",
    "        dJdgamma = np.mean(dJdbnx * xhat, axis=-1)[:, None]\n",
    "        dJdx = (1.0 - 1.0/bs) * (1.0/v - u**2 / v**3 / bs) * gamma * dJdbnx\n",
    "        \n",
    "        return dJdbeta, dJdgamma, dJdx\n",
    "                  \n",
    "                  \n",
    "    # Activation function\n",
    "    def activation_func(self,i):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            np.copyto(self.net[i].a, self.sigmoid(self.net[i].bnz))\n",
    "        elif self.activation == \"tanh\":\n",
    "            np.copyto(self.net[i].a, self.tanh(self.net[i].bnz))\n",
    "        elif self.activation == \"swish\":\n",
    "            np.copyto(self.net[i].a, self.swish(self.net[i].bnz))\n",
    "        elif self.activation == \"LeakyReLU\":\n",
    "            np.copyto(self.net[i].a, self.LeakyReLU(self.net[i].bnz))\n",
    "        else:\n",
    "            np.copyto(self.net[i].a, self.ReLU(self.net[i].bnz))\n",
    "    def activation_func_prime(self,i):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            np.copyto(self.net[i].dJdbnz,(self.net[i].dJda * self.sigmoidPrime(self.net[i].a))) \n",
    "        elif self.activation == \"tanh\":\n",
    "            np.copyto(self.net[i].dJdbnz,(self.net[i].dJda * self.tanhPrime(self.net[i].a))) \n",
    "        elif self.activation == \"swish\":\n",
    "            np.copyto(self.net[i].dJdbnz,(self.net[i].dJda * self.swishPrime(self.net[i].a, self.net[i].bnz)))\n",
    "        elif self.activation == \"LeakyReLU\":\n",
    "            np.copyto(self.net[i].dJdbnz,(self.net[i].dJda * self.LeakyReLUPrime(self.net[i].a,self.leakyRate))) \n",
    "        else:\n",
    "            np.copyto(self.net[i].dJdbnz,(self.net[i].dJda * self.ReLUPrime(self.net[i].a)))       \n",
    "                  \n",
    "    def softmax(self, a):\n",
    "        return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    \n",
    "    # Sigmoid activation function: z --> a\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-1.0*z))\n",
    "    \n",
    "    # a --> dadz\n",
    "    def sigmoidPrime(self, a):\n",
    "        return a * (1.0 - a)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    def tanhPrime(self, a):\n",
    "        return (1.0 - a ** 2) # Derivative of tanh is (1.0 - tanh ** 2)\n",
    "    \n",
    "    def swish(self, z):\n",
    "        return z * self.sigmoid(z)\n",
    "    \n",
    "    def swishPrime(self, a, z):\n",
    "        return a + self.sigmoid(z) * (1.0 - a) \n",
    "    \n",
    "    def ReLU(self, z):\n",
    "        a = np.copy(z)\n",
    "        a[a<0] = 0.0\n",
    "        return a\n",
    "    \n",
    "    def ReLUPrime(self, a):\n",
    "        dadz = np.copy(a)\n",
    "        dadz[a>0] = 1.0\n",
    "        return dadz\n",
    "    \n",
    "    def LeakyReLU(self, z, leakyrate):\n",
    "        a = np.copy(z)\n",
    "        (rows, cols) = a.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                a[i, j] = z[i, j] if z[i,j] > 0 else (leakyrate * z[i, j])\n",
    "        return a\n",
    "    \n",
    "    def LeakyReLUPrime(self, a, leakyrate):\n",
    "        dadz = np.copy(a)\n",
    "        (rows, cols) = dadz.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                dadz[i, j] = 1.0 if a[i,j] > 0 else leakyrate\n",
    "        return dadz\n",
    "    \n",
    "    def loss(self, y, eta):\n",
    "                  \n",
    "        self.y_predict.fill(0)\n",
    "        for i in range(self.bs):\n",
    "            self.y_predict[y[i],i] = 1\n",
    "        self.J.append(-1.0 * np.sum(self.y_predict * np.log(self.p) / self.bs))\n",
    "        \n",
    "        # L2 Regularization\n",
    "        L2 = 0.0 \n",
    "        for i in range(1, len(self.net)):\n",
    "            L2 += np.sum(self.net[i].W ** 2)              \n",
    "        self.L2_regularization.append(eta / 2 * L2) # Only MLP do \n",
    "        \n",
    "        return\n",
    "                  \n",
    "    # Loss function for validation\n",
    "    def loss_val(self, y, eta):\n",
    "        \n",
    "        self.y_predict.fill(0)\n",
    "        for i in range(self.bs):\n",
    "            self.y_predict[y[i], i] = 1     \n",
    "        self.J_val.append(-1.0 * np.sum(self.y_predict * np.log(self.p) / self.bs))\n",
    "        \n",
    "        return\n",
    "    def backprop(self):\n",
    "        \n",
    "        self.dJdp = 1.0 / (1.0 - self.y_predict - self.p)\n",
    "        dpda = np.array([[self.p[i, :] * (1.0-self.p[j, :]) if i == j\n",
    "                          else -1 * self.p[i, :] * self.p[j, :]\n",
    "                          for i in range(self.p.shape[0])]\n",
    "                         for j in range(self.p.shape[0])])\n",
    "        for i in range(self.bs):\n",
    "            self.net[-1].dJda[:, i] = np.dot(dpda[:, :, i], self.dJdp[:, i])\n",
    "            \n",
    "        for i in range(len(self.net)-1, 0, -1):\n",
    "            \n",
    "            self.activation_func_prime(i=i)\n",
    "           \n",
    "            dJdbeta, dJdgamma, dJdBNinput = self.batch_norm_prime(self.net[i].dJdbnz,\n",
    "                                                                  self.net[i].z,\n",
    "                                                                  self.net[i].gamma,\n",
    "                                                                  self.net[i].beta,\n",
    "                                                                  0.01)  \n",
    "            np.copyto(self.net[i].dJdbeta, dJdbeta)\n",
    "            np.copyto(self.net[i].dJdgamma, dJdgamma)\n",
    "            np.copyto(self.net[i].dJdz, dJdBNinput) \n",
    "            \n",
    "            np.copyto(self.net[i].dJdb,\n",
    "                      np.mean(self.net[i].dJdz, axis = -1)[:, None])         \n",
    "            np.copyto(self.net[i].dJdW,\n",
    "                      np.dot(self.net[i].dJdz, self.net[i].i.T) / self.bs)  \n",
    "            np.copyto(self.net[i].dJdi,\n",
    "                      np.dot(self.net[i].W.T, self.net[i].dJdz))\n",
    "            \n",
    "            # Copy gradient at the input to be the output gradient of the previous layer\n",
    "            np.copyto(self.net[i-1].dJda, self.net[i].dJdi)\n",
    "        \n",
    "        # Layer 0 does nothing but passing gradients backward\n",
    "        np.copyto(self.net[0].dJdbnz, self.net[0].dJda)\n",
    "        np.copyto(self.net[0].dJdz, self.net[0].dJdbnz)\n",
    "        np.copyto(self.net[0].dJdi, self.net[0].dJdz) \n",
    "        return\n",
    "    # Update parameters\n",
    "    def update(self, lr, eta):\n",
    "        \n",
    "        # Update W, b, gamma, beta with Weight Decay from Layer 1 to the last\n",
    "        # Layer 0 has no parameters\n",
    "        for i in range(1, len(self.net)):\n",
    "            np.copyto(self.net[i].W,\n",
    "                      (1.0 - eta * lr) * self.net[i].W - lr*self.net[i].dJdW)\n",
    "            np.copyto(self.net[i].b,\n",
    "                      (1.0 - eta * lr) * self.net[i].b - lr*self.net[i].dJdb)\n",
    "            np.copyto(self.net[i].gamma,\n",
    "                      (1.0 - eta * lr) * self.net[i].gamma - lr*self.net[i].dJdgamma)\n",
    "            np.copyto(self.net[i].beta,\n",
    "                      (1.0 - eta * lr) * self.net[i].beta - lr*self.net[i].dJdbeta)\n",
    "        return\n",
    "    \n",
    "    # Train MLP alone. For a CNN, training is via the CNN instance\n",
    "    def train(self, train_x, train_y, epoch_count, lr, eta):\n",
    "        \n",
    "        for e in range(epoch_count):\n",
    "            # print (\"Epoch \", e)\n",
    "            for i in range(train_x.shape[1]//self.bs):\n",
    "                x = train_x[:, i*self.bs:(i+1)*self.bs]\n",
    "                y = train_y[i*self.bs:(i+1)*self.bs]\n",
    "                self.forward(x)\n",
    "                self.loss(y, eta)\n",
    "                self.backprop()\n",
    "                self.update(lr, eta)           \n",
    "        return  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
