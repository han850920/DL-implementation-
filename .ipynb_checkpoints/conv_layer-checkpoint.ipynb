{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class cl(object):\n",
    "    def __init__(self,\n",
    "                 bs,\n",
    "                 i_ch,#input channel\n",
    "                 i_h,#input height\n",
    "                 i_w,#input width\n",
    "                 k_num,#kernel number equals to number of output channel\n",
    "                 k_h,#kernel height\n",
    "                 k_w,#kernel width\n",
    "                 stride = 1, # conv move #stride per step\n",
    "                 zp = 0,#zero padding\n",
    "                 maxPoolingHeight = 2,\n",
    "                 maxPoolingWidth = 2,\n",
    "                 leakyrate = 0.1,\n",
    "                 activation_func = \"LeakyReLu\"\n",
    "                ):\n",
    "        self.bs = bs\n",
    "        self.i_ch = i_ch\n",
    "        self.i_h = i_h\n",
    "        self.i_w = i_w\n",
    "        self.k_num = k_num\n",
    "        self.k_h = k_h\n",
    "        self.k_w = k_w\n",
    "        self.stride = stride\n",
    "        self.zp = zp\n",
    "        self.maxPoolingHeight = maxPoolingHeight\n",
    "        self.maxPoolingWidth = maxPoolingWidth\n",
    "        self.leakyrate = leakyrate\n",
    "        self.activation_func = activation_func\n",
    "        \n",
    "        self.ai = np.zeros((bs, i_ch, i_h+2*zp, i_w+2*zp), dtype = 'float64')\n",
    "        self.z = np.zeros((bs, k_num, (i_h - k_h + 1)//stride, (i_w - k_w + 1)//stride), dtype = 'float64') # why should i mod stride?\n",
    "        self.bnz =  np.zeros(self.z.shape,dtype = 'float64') # is it essential?\n",
    "        self.r =  np.zeros(self.z.shape,dtype = 'float64') # the result after activation\n",
    "        self.max_pos01 = np.zeros(self.r.shape,dtype = 'int32') # record the maxpooling position, set it by 1 ,others by 0\n",
    "        self.ao = np.zeros((bs, k_num, (i_h - k_h + 1)//stride//self.maxPoolingHeight, (i_w - k_w + 1)//stride//self.maxPoolingWidth), dtype = \"float64\")\n",
    "        self.W = np.random.randn(k_num, i_ch, k_h, k_w).astype(\"float64\")\n",
    "        self.b = np.random.randn(k_num,1).astype(\"float64\")\n",
    "        \n",
    "        self.gamma = np.ones(self.z.shape[1:], dtype='float64')\n",
    "        self.beta = np.zeros(self.z.shape[1:], dtype='float64')\n",
    "        \n",
    "        self.dJdai = np.zeros(self.ai.shape, dtype='float64')\n",
    "        self.dJdz = np.zeros(self.z.shape, dtype='float64')\n",
    "        self.dJdbnz = np.zeros(self.bnz.shape, dtype='float64')\n",
    "        self.dJdr = np.zeros(self.r.shape, dtype='float64')\n",
    "        self.dJdao = np.zeros(self.ao.shape, dtype='float64')\n",
    "        self.dJdW = np.zeros(self.W.shape, dtype='float64')\n",
    "        self.dJdb = np.zeros(self.b.shape, dtype='float64')\n",
    "        self.dJdgamma = np.zeros(self.gamma.shape, dtype='float64')\n",
    "        self.dJdbeta = np.zeros(self.beta.shape, dtype='float64')\n",
    "                           \n",
    "    def conv_2d(self):\n",
    "        (i_bs, i_ch, i_rows, i_cols) = self.ai.shape # Input data\n",
    "        (k_num, k_ch, k_rows, k_cols) = self.W.shape # Kernels\n",
    "        (z_bs, z_ch, z_rows, z_cols) = self.z.shape   # Output z_ch == k_num\n",
    "        s = self.stride\n",
    "\n",
    "        for b in range(z_bs):\n",
    "            for c in range(z_ch):\n",
    "                for i in range(z_rows):\n",
    "                       for j in range(z_cols):\n",
    "                            self.z[b,c,i,j] = np.sum(np.multiply(self.W[c,:,:,:],\n",
    "                                                    self.ai[b, : , i*s:i*s+k_rows , j*s:j*s+k_cols ])\n",
    "                                                   )+self.b[c]\n",
    "        return\n",
    "    def conv_prime(self):\n",
    "        (z_bs, z_ch, z_rows, z_cols) = self.dJdz.shape\n",
    "        (a_bs, a_ch, a_rows, a_cols) = self.dJdai.shape\n",
    "        (k_num, k_ch, k_rows, k_cols) = self.dJdW.shape\n",
    "        s = self.stride # self.stride\n",
    "        self.dJdai.fill(0.0)\n",
    "        self.dJdW.fill(0.0)\n",
    "        self.dJdb.fill(0.0)\n",
    "\n",
    "        for b in range(z_bs):\n",
    "            for c in range(z_ch):\n",
    "                for i in range(z_rows):\n",
    "                    for j in range(z_cols):\n",
    "                        dJdz_value = self.dJdz[b, c, i, j]\n",
    "                        self.dJdai[b, :, i*s:i*s+k_rows, j*s:j*s+k_cols] += dJdz_value * self.W[c, :, :, :]\n",
    "        for k in range(k_num):\n",
    "            for c in range(k_ch):\n",
    "                for i in range(k_rows):\n",
    "                    for j in range(k_cols):\n",
    "                        self.dJdW[k,c,i,j] = np.sum(self.ai[:,c,i:i+z_rows*s:s,j:j+z_cols*s:s] *\n",
    "                                                    self.dJdz[:,k,:,:])/self.bs\n",
    "        for c in range(z_ch):\n",
    "            self.dJdb[c] = np.sum(self.dJdz[:,c,:,:])/self.bs\n",
    "\n",
    "        return\n",
    "\n",
    "    # All activation functions produce  z --> zr  \n",
    "    def activation(self):\n",
    "        if self.activation_func == \"tanh\":\n",
    "            self.tanh()\n",
    "        elif self.activation_func == \"swish\":\n",
    "            self.swish()\n",
    "        elif self.activation_func == \"sigmoid\":\n",
    "            self.sigmoid()\n",
    "        else:\n",
    "            self.LeakyReLU()\n",
    "    def activation_prime(self):\n",
    "        if self.activation_func == \"tanh\":\n",
    "            self.tanh_prime()\n",
    "        elif self.activation_func == \"swish\":\n",
    "            self.swish_prime()\n",
    "        elif self.activation_func == \"sigmoid\":\n",
    "            self.sigmoid_prime()\n",
    "        else:\n",
    "            self.LeakyReLU_prime()\n",
    "\n",
    "    def LeakyReLU(self): # if z>0 then r = 1.0*self.bnz else r = self.lekyrate * self.bnz -> 即使小於等於0也值輸出 避免神經元死亡 \n",
    "        np.copyto(self.r, np.where(self.z > 0, 1.0 * self.bnz, self.leakyrate * self.bnz))\n",
    "        return\n",
    "\n",
    "    def LeakyReLU_prime(self): \n",
    "        np.copyto(self.dJdbnz, np.where(self.z > 0, 1.0 * self.dJdr, self.leakyrate * self.dJdr))\n",
    "        return\n",
    "\n",
    "    def tanh(self):#會有梯度消失的問題\n",
    "        np.copyto(self.r, (np.exp(self.zr) - np.exp(-self.zr)) / (np.exp(self.zr) + np.exp(-self.zr)))\n",
    "        return\n",
    "\n",
    "    def tanh_prime(self):\n",
    "        np.copyto(self.dJdzr, (1.0 - self.r ** 2))\n",
    "        return\n",
    "\n",
    "    def swish(self): #結果比ReLU好 但效能比ReLU略差\n",
    "        np.copyto(self.zr, self.z * self.sigmoid(self.z))\n",
    "        return\n",
    "\n",
    "    def swish_prime(self):\n",
    "        np.copyto(self.dJdz,\n",
    "                  self.zr + self.sigmoid(self.z) * (1.0 - self.zr))\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def batch_norm(self, eps=0.01):  \n",
    "        \n",
    "        x = self.z\n",
    "        bs = x.shape[0]\n",
    "        mu = 1.0/bs * np.sum(x, axis=0)[None, :, :, :]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=0)[None, :, :, :]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        np.copyto(self.bnz,\n",
    "                  self.gamma * xhat + self.beta)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # (dJdr, zr, gamma) --> (dJdbeta, dJdgamma, dJdzr)\n",
    "    def batch_norm_prime(self, eps = 0.01):\n",
    "        \n",
    "        x = self.z\n",
    "        bs = x.shape[0]\n",
    "        mu = 1.0/bs * np.sum(x, axis=0)[None, :, :, :]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=0)[None, :, :, :]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        self.dJdbeta = np.mean(self.dJdbnz, axis=0)[None, :, :, :]\n",
    "        self.dJdgamma = np.mean(self.dJdbnz * xhat, axis=0)[None, :, :, :]\n",
    "        self.dJdz = (1.0 - 1.0/bs) * (1.0/v - u**2/v**3/bs) * self.gamma * self.dJdbnz\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def batch_norm_pass(self, eps=0.01):\n",
    "        \n",
    "        np.copyto(self.r, self.zr)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def batch_norm_prime_pass(self, eps = 0.01):\n",
    "        \n",
    "        np.copyto(self.dJdzr, self.dJdr)     \n",
    "        return   \n",
    "\n",
    "\n",
    "    def maxPool(self):\n",
    "        (r_bs, r_ch, r_rows, r_cols) = self.r.shape\n",
    "        (a_bs, a_ch, a_rows, a_cols) = self.ao.shape\n",
    "        self.max_pos01.fill(0) # record from which is the maximum so we can backprop\n",
    "        for b in range(a_bs):\n",
    "            for c in range(a_ch):\n",
    "                for i in range(a_rows):\n",
    "                    for j in range(a_cols):\n",
    "                        pooling_src = self.r[b,\n",
    "                                             c,\n",
    "                                             i*self.maxPoolingHeight:(i+1)*self.maxPoolingHeight,\n",
    "                                             j*self.maxPoolingWidth:(j+1)*self.maxPoolingWidth]\n",
    "                        self.ao[b,c,i,j] = np.max(pooling_src)\n",
    "                        max_pos = np.unravel_index(np.argmax(pooling_src),np.shape(pooling_src))\n",
    "                        self.max_pos01[b, c, i*self.maxPoolingHeight+max_pos[0], j*self.maxPoolingWidth+max_pos[1]] = 1\n",
    "        return\n",
    "    #computing dJdr\n",
    "    def maxPool_prime(self):\n",
    "        (r_bs,r_ch,r_rows, r_cols)=self.dJdr.shape\n",
    "        for b in range(r_bs):\n",
    "            for c in range(r_ch):\n",
    "                for i in range(r_rows):\n",
    "                    for j in range(r_cols):\n",
    "                        self.dJdr[b,c,i,j] = (self.dJdao[b,c,i//self.maxPoolingHeight,j//self.maxPoolingWidth]\n",
    "                                              if self.max_pos01[b,c,i,j] ==1\n",
    "                                              else 0.0)\n",
    "        return\n",
    "\n",
    "\n",
    "    # a(i-1) --conv--> z --activ--> r --maxpool--> a(i)\n",
    "    def forward(self):\n",
    "        self.conv_2d()\n",
    "        self.batch_norm()\n",
    "        self.activation()\n",
    "        self.maxPool()\n",
    "        return\n",
    "    def backprop(self):\n",
    "        self.maxPool_prime()\n",
    "        self.activation_prime()\n",
    "        self.batch_norm_prime()\n",
    "        self.conv_prime()\n",
    "        return\n",
    "    def update(self,lr,eta):\n",
    "        self.W = (1.0 - eta * lr) * self.W - lr * self.dJdW\n",
    "        self.b = (1.0 - eta * lr) * self.b -lr * self.dJdb\n",
    "        self.gamma = (1.0 - eta * lr) * self.gamma -lr * self.dJdgamma\n",
    "        self.beta = (1.0 - eta * lr) * self.beta -lr * self.dJdbeta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
